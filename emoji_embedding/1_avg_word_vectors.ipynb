{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633c5b7-c00a-45cb-9fff-e9a8c6f098e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46975738-98aa-4b36-aa67-624a6c63f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e4aff-be43-400d-bcd5-f9d52ed10ba2",
   "metadata": {},
   "source": [
    "#### load merged and partly processed emoji data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d6d35-e30f-4c06-973c-3872e4d4df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed/emoji_descriptions.csv\")\n",
    "df.emjpd_aliases = df.emjpd_aliases.apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    ")\n",
    "df.emjpd_aliases = df.emjpd_aliases.apply(lambda x: \" \".join(x))\n",
    "for col in [\n",
    "    \"emjpd_emoji_name_og\",\n",
    "    \"hemj_emoji_name_og\",\n",
    "    \"emjpd_aliases\",\n",
    "    \"emjpd_full_description\",\n",
    "    \"emjpd_description_main\",\n",
    "    \"emjpd_description_side\",\n",
    "    \"hemj_emoji_description\",\n",
    "    \"emjpd_usage_info\",\n",
    "]:\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "df[\"emoji_name_og\"] = (\n",
    "    df.emjpd_emoji_name_og.fillna(\"\") + \" \" + df.hemj_emoji_name_og.fillna(\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074c64c-6231-4e01-a898-3ece582150ea",
   "metadata": {},
   "source": [
    "### load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d7025-185b-4025-8c8e-adeefc67d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model):\n",
    "    \"\"\"Load GloVe Vectors from Gensim\n",
    "    Params:\n",
    "        - model {str}: string specifying the model, possibilities include:\n",
    "            - glove-wiki-gigaword-200\n",
    "            - glove-twitter-200\n",
    "            - word2vec-google-news-300\n",
    "            - glove-wiki-gigaword-300\n",
    "    Return:\n",
    "        - wv_from_bin {gensim.models.keyedvectors.KeyedVectors}: Embeddings of all words\n",
    "    \"\"\"\n",
    "    wv_from_bin = api.load(model)\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c5a5e-7e88-4549-b7e3-57da4dd3ff36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"glove-twitter-200\"\n",
    "emb = load_embedding_model(model)\n",
    "emb_vocabulary = set(emb.index_to_key)\n",
    "default = np.zeros(emb.get_vector(\"hello\").shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eeb84-6794-4f24-b811-b1492a3706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(text):\n",
    "    \"\"\"Returns list of all words (and numbers) in a given text. \n",
    "    Special characters and punctuation are ignored. \n",
    "    \"\"\"\n",
    "    print(text, \"\\n\")\n",
    "    return re.compile(\"\\w+\").findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd8209-8f24-4b96-a317-01186bbc3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, default=np.zeros(200, dtype=np.float32)):\n",
    "    \"\"\"Given a string or a list of words (in lower case)\n",
    "    the average word vector for all words that can be found \n",
    "    in the embedding vocabulary is returned. Words that can't be found \n",
    "    are not included in the average calculation. If no word is in the \n",
    "    embedding vocaublary the default (vector of 0s) is returned. \n",
    "    \n",
    "    Returns: {np.array}\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    if isinstance(text, str) and len(text) > 0:\n",
    "        words = re.compile(\"\\w+\").findall(text)\n",
    "    elif isinstance(text, list):\n",
    "        words = text\n",
    "    words = [w for w in words if w in emb_vocabulary]\n",
    "    if len(words) > 0:\n",
    "        embeddings = [emb.get_vector(w) for w in words]\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5a7a5-6e31-4093-a538-0508838ca796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(weighting, df):\n",
    "    \"\"\"Returns average word embedding for emojis given weighting rule. For \n",
    "    each column specified in weighting the average embedding will be calculated. \n",
    "    The embeddings of the columns are averaged with the specified weights \n",
    "    (weights don't have to sum to 1). \n",
    "    \n",
    "    Params: \n",
    "        - weighting {dictionary}: dictionary containing weights for columns that are to \n",
    "                    be considered in the weighted average calculation of word embeddings\n",
    "        - df {pd.DataFrame}: Dataframe with different emojis in each row and their descriptions, \n",
    "                    in particular it has the columns specified in weighting. The descriptions are taken \n",
    "                    for the word embedding average calculation. \n",
    "                    \n",
    "    Returns: \n",
    "        - {pd.Series}: pandas Series of np.arrays that contain the word embeddings for the emojis\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    total_weights = []\n",
    "    for col, weight in weighting.items():\n",
    "        vectors = df[col].apply(get_vector)\n",
    "        addition = weight * vectors\n",
    "        result.append(addition)\n",
    "\n",
    "        indictaion = weight * (vectors.apply(sum) != 0)\n",
    "        total_weights.append(indictaion)\n",
    "    result = pd.concat(result, axis=1).sum(axis=1)\n",
    "    total_weights = pd.concat(total_weights, axis=1).sum(axis=1)\n",
    "    return result / total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfa994-228b-4ae2-b716-6055d0f26a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting = {\n",
    "    \"emoji_name_og\": 30,\n",
    "    \"emjpd_aliases\": 15,\n",
    "    \"emjpd_description_main\": 35,\n",
    "    \"emjpd_description_side\": 5,\n",
    "    \"hemj_emoji_description\": 15,\n",
    "}\n",
    "\n",
    "embeddings = get_embedding(weighting, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59eb932-f5de-4635-9ac8-7f5d31871a55",
   "metadata": {},
   "source": [
    "### plausibility check\n",
    "\n",
    "Check whether cosine similarity between similar emojis is indeed higher than unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0712e-1a43-48cd-89d2-841e59715db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ana = df[[\"emoji_char\"]].copy()\n",
    "ana[\"embeddings\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3e5ab-8bef-4b8f-a8cf-a9a208b1fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(ana, idx):\n",
    "    return torch.Tensor(ana.iloc[idx].embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aaeae6-7f51-40bb-bbca-7727e510c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_embedding(ana, 1866)\n",
    "b = get_embedding(ana, 1638)\n",
    "c = get_embedding(ana, 1498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1db98-fc9d-4dbd-b8bb-4303a50be988",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = nn.CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cefe38-88a9-4651-84b4-6b300d6bf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0403fe-9c2b-475f-8fa1-ac3b985c82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ana.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e02b2-6441-4392-b410-225903ddff61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
